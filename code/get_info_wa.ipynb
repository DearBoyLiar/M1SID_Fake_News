{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base librairies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Content extraction\n",
    "import glob\n",
    "\n",
    "# Pr√©diction librairies\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Network part\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(name):\n",
    "    dictio = {}\n",
    "    list_of_files = glob.glob('../data/news/'+name+'/*.txt')        \n",
    "    for i,file_name in enumerate(list_of_files):\n",
    "        id_article = file_name[:-4].split('/')[4]\n",
    "        dictio[int(id_article)] = open(file_name,'r').read()\n",
    "    return dictio\n",
    "\n",
    "def get_words(content):\n",
    "    vectorizer = TfidfVectorizer(min_df=1,lowercase=False,stop_words='english')\n",
    "    vectorizer.fit_transform(content.split('.'))\n",
    "    return vectorizer.get_feature_names()\n",
    "\n",
    "def get_features(df,vocab=None):\n",
    "    df['len_content'] = df['content'].apply(lambda x : len(x))\n",
    "    \n",
    "    df['len_title'] = df['content'].apply(lambda x: len(x.split('\\n')[0]))\n",
    "    global_vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',encoding='utf-8', input='content',\n",
    "                                        lowercase=True, max_df=0.75, max_features=None, min_df=8,\n",
    "                                        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
    "                                        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
    "                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
    "                                        vocabulary=None)\n",
    "    global_vectorizer.fit_transform(df['content'])\n",
    "    pertinent_vocab = global_vectorizer.get_feature_names()\n",
    "    \n",
    "    article_vocab = [get_words(content) for content in df['content']]\n",
    "    df['nb_relevant'] = [len (set(vocab) & set(pertinent_vocab)) for vocab in article_vocab]\n",
    "    \n",
    "    golbal_count_vectorizer = CountVectorizer(min_df=6,max_df=0.75,stop_words='english',max_features=400,vocabulary=vocab)\n",
    "    X = golbal_count_vectorizer.fit_transform(df['content'])\n",
    "    df_words = pd.DataFrame(X.toarray(),index=df.index,columns=golbal_count_vectorizer.get_feature_names())\n",
    "    df_words['len_content'], df_words['nb_relevant'],df_words['len_title'] = df['len_content'],df['nb_relevant'],df['len_title']\n",
    "    \n",
    "    df_words['nb_words'] = [len(vocab) for vocab in article_vocab]\n",
    "    upper_case_words = []\n",
    "    for vocab in article_vocab:\n",
    "        upper_case_words.append(len([word for word in vocab if word.isupper()]))\n",
    "    df_words['nb_uppercase_words'] = upper_case_words\n",
    "    \n",
    "    i = df_words.index\n",
    "    df_words['nb_entity'] =[0]*len(df_words)\n",
    "    for index,vocab in zip(i,article_vocab):\n",
    "        for word in vocab:\n",
    "            if (any(x.isupper() for x in word)) or word.isupper():\n",
    "                df_words['nb_entity'].loc[index] += 1\n",
    "                \n",
    "    return df_words,golbal_count_vectorizer.get_feature_names()\n",
    "    \n",
    "\n",
    "def get_Y_train():\n",
    "    articles = open('../data/labels_training.txt','r').readlines()[1:]\n",
    "    id_articles = [int(art.split(',')[0]) for art in articles]\n",
    "    return pd.DataFrame([int(art.split(',')[1][:-1]) for art in articles],columns=['fake_news'],index=id_articles)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return metrics.mean_squared_error(y_true, y_pred) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get articles for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_train = get_articles('training')\n",
    "rough_test = get_articles('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame.from_dict(rough_train,orient='index',columns=['content'])\n",
    "test = pd.DataFrame.from_dict(rough_test,orient='index',columns=['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,vocab= get_features(train)\n",
    "df_test,vocab = get_features(test,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(df_train['len_content'],'.')\n",
    "plt.title('Number of caracters per article')\n",
    "plt.xlabel('Article id')\n",
    "plt.ylabel('Nb caracters')\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(df_train['nb_relevant'],'.')\n",
    "plt.title('Number of relevant words per article')\n",
    "plt.xlabel('Article id')\n",
    "plt.ylabel('Nb relevants words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best parameters (pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" TfIdfVectorizer parameter\"\"\"\n",
    "max_dfs = [0.75]\n",
    "min_dfs = [7]\n",
    "ngrams_range = [(1, 1), (1, 2)]\n",
    "\n",
    "\n",
    "\"\"\"Random Forest parameters\"\"\"\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 400, stop = 1600, num = 4)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto','sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(25, 100, num = 4)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', RandomForestClassifier()\n",
    "    ),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': max_dfs,\n",
    "    'tfidf__min_df': min_dfs,\n",
    "    'tfidf__ngram_range': ngrams_range,\n",
    "    'clf__n_estimators': n_estimators,\n",
    "    'clf__max_features': max_features,\n",
    "    'clf__max_depth': max_depth,\n",
    "    'clf__min_samples_split': min_samples_split,\n",
    "    'clf__min_samples_leaf': min_samples_leaf\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=5, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit( train.sort_index()['content'], get_Y_train().sort_index())\n",
    "\n",
    "print (grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's prediction time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.sort_index()\n",
    "Y_train = get_Y_train().sort_index()\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real y-train distribution\n",
    "sns.distplot(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on a model and best parameters, get a cross val score\n",
    "clf = RandomForestClassifier(n_estimators= 1600,min_samples_split= 2,min_samples_leaf=1,max_features='auto',max_depth=70,bootstrap=False)\n",
    "np.mean(cross_val_score(clf,X_train,Y_train,cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators= 1000,min_samples_split= 5,min_samples_leaf=2,max_features='sqrt',max_depth=100,bootstrap=False)\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = str(datetime.datetime.now())[:19]\n",
    "with open('../data/submissions/bogota_mates_submission_'+now+'.txt', 'w') as f:\n",
    "    f.write('doc,class\\n')\n",
    "    for value,doc in zip(Y_test,X_test.index):\n",
    "        f.write(str(doc)+ ',' + str(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/newsUser.txt','r')\n",
    "nb_news = pd.DataFrame([line.split() for line in f.readlines()],columns=['id_article','id_user','weight'])\n",
    "f = open('../data/UserUser.txt','r')\n",
    "follows = pd.DataFrame([line.split() for line in f.readlines()],columns=['follower','followed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_news['id_article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "for i,val in enumerate(nb_news['id_article']):\n",
    "    g.add_edges_from([(nb_news['id_article'][i],nb_news['id_user'][i])],weight=nb_news['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = pd.Series([node[1] for node in g.degree])\n",
    "sns.distplot(degree,kde=False,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree\n",
    "degree_centrality = nx.degree_centrality(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
