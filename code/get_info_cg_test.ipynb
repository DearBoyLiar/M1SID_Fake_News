{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgbm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import sklearn.decomposition as skld\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lists(path):\n",
    "    files = glob.glob('../data/news/'+path+'/*.txt')   \n",
    "    id_articles = []\n",
    "    articles_contents = []\n",
    "    for file in files:\n",
    "        id_article = (file.split(\"\\\\\")[1]).split('.')[0];\n",
    "        f = open('../data/news/'+path+'/'+id_article+'.txt','r', encoding='utf-8').read()\n",
    "        id_articles.append(int(id_article))\n",
    "        articles_contents.append(f)\n",
    "    return id_articles, articles_contents\n",
    "\n",
    "X_train_ids, X_train_contents = create_lists('training')\n",
    "X_test_ids, X_test_contents = create_lists('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y_train():\n",
    "    true_val = []\n",
    "    indexes = []\n",
    "    articles = open('../data/labels_training.txt','r').readlines()[1:]\n",
    "    for i in articles:\n",
    "        true_val.append(int(i.split(',')[1][:-1]))\n",
    "        indexes.append(i.split(',')[0])\n",
    "    return pd.DataFrame(true_val,columns=['fake_news'],index=indexes)\n",
    "\n",
    "Y_train = get_Y_train().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentense2cleanTokens(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = \"\".join([x if x.isalpha() else \" \" for x in sent])\n",
    "    sent = \" \".join(sent.split())\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in X_train_contents:\n",
    "    X_train.append(sentense2cleanTokens(i))\n",
    "    \n",
    "X_test = []\n",
    "for j in X_test_contents:\n",
    "    X_test.append(sentense2cleanTokens(j))\n",
    "    \n",
    "Y_train = list(Y_train['fake_news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peut etre a remettre 10 en min_df !\n",
    "vec = TfidfVectorizer(max_df=0.7, min_df=0.1, max_features=100000, stop_words='english')\n",
    "#vec = CountVectorizer(max_df=0.7, min_df=10, max_features=100000, stop_words='english') \n",
    "X = vec.fit_transform(X_train + X_test)\n",
    "df = pd.DataFrame(X.toarray(), index=X_train_ids + X_test_ids, columns=vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Ajout plus grande phrase, nombre de phrases</b></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_grande_phrase = []\n",
    "number_sentences = []\n",
    "for i in X_train_contents:\n",
    "    maxi_phrase = 0\n",
    "    nb_sent = 0\n",
    "    for sentence in i.split(\".\"):\n",
    "        nb_sent += 1\n",
    "        if len(sentence) > maxi_phrase:\n",
    "            maxi_phrase = len(sentence)\n",
    "    plus_grande_phrase.append(maxi_phrase)\n",
    "    number_sentences.append(nb_sent)\n",
    "for i in X_test_contents:\n",
    "    maxi_phrase = 0\n",
    "    nb_sent = 0\n",
    "    for sentence in i.split(\".\"):\n",
    "        nb_sent += 1\n",
    "        if len(sentence) > maxi_phrase:\n",
    "            maxi_phrase = len(sentence)\n",
    "    plus_grande_phrase.append(maxi_phrase)\n",
    "    number_sentences.append(nb_sent)\n",
    "\n",
    "df['plus_grande_phrase']= plus_grande_phrase\n",
    "df['nb_sent'] = number_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### partie william"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longueur_article = []\n",
    "for article in X_train:\n",
    "    longueur_article.append(len(article))\n",
    "for article in X_test:\n",
    "    longueur_article.append(len(article))\n",
    "df['len_article'] = longueur_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longueur_titre = []\n",
    "for article in X_train_contents:\n",
    "    longueur_titre.append(len(article.split('\\n')[0]))\n",
    "for article in X_test_contents:\n",
    "    longueur_titre.append(len(article.split('\\n')[0]))\n",
    "df['len_title'] = longueur_titre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word = []\n",
    "nb_entity = []\n",
    "for article in X_train_contents:\n",
    "    cpt = 0\n",
    "    for word in article.split(' '):\n",
    "        if word[:1].isupper() or word.isupper():\n",
    "             cpt += 1\n",
    "    nb_word.append(len(article.split(' ')))\n",
    "    nb_entity.append(cpt)\n",
    "for article in X_test_contents:\n",
    "    cpt = 0\n",
    "    for word in article.split(' '):\n",
    "        if word[:1].isupper() or word.isupper():\n",
    "             cpt += 1\n",
    "    nb_word.append(len(article.split(' ')))\n",
    "    nb_entity.append(cpt)\n",
    "    \n",
    "df['nb_word'] = nb_word\n",
    "#C'est plutôt le nombre de mots commençant par une majuscule en fait\n",
    "df['nb_entity'] = nb_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Partie test de truc noul </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sent = []\n",
    "for article in X_train_contents:\n",
    "    last_sent.append(len(article.split('\\n')[-1]))\n",
    "for article in X_test_contents:\n",
    "    last_sent.append(len(article.split('\\n')[-1]))\n",
    "df['last_sent'] = last_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Ajout en fonction du nombre de type de mot</b></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "nb_nnp = []\n",
    "for article in X_train:\n",
    "    cpt = 0\n",
    "    for i in pos_tag(word_tokenize(article)):\n",
    "        if i[1] == 'NNP':\n",
    "            cpt += 1\n",
    "    nb_nnp.append(cpt)\n",
    "for article in X_test:\n",
    "    cpt = 0\n",
    "    for i in pos_tag(word_tokenize(article)):\n",
    "        if i[1] == 'NNP':\n",
    "            cpt += 1\n",
    "    nb_nnp.append(cpt)\n",
    "df['nb_nnp'] = nb_nnp\"\"\"\n",
    "\n",
    "##Pas pertinent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Ajout des infos users</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsUsers = open('../data/newsUser.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_articles = []\n",
    "id_users = []\n",
    "nb_propa = []\n",
    "for i in newsUsers:\n",
    "    id_articles.append(str(i.split(\"\\t\")[0]))\n",
    "    id_users.append(str(i.split(\"\\t\")[1]))\n",
    "    nb_propa.append(int((i.split(\"\\t\")[2]).split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_users = {}\n",
    "for i in range(len(id_articles)):\n",
    "    users_propa = {}\n",
    "    if id_articles[i] in article_users.keys():\n",
    "        users_propa = article_users[id_articles[i]]\n",
    "    users_propa[id_users[i]] = nb_propa[i]\n",
    "    article_users[id_articles[i]] = users_propa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>article_users : dico de la forme clé id_article : dico associé clé id_user values nb de propagation de cet article par cet user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UsersUsers = open('../data/UserUser.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_users_followers = []\n",
    "id_users_followed = []\n",
    "for i in UsersUsers:\n",
    "    id_users_followers.append(str(i.split(\"\\t\")[0]))\n",
    "    id_users_followed.append(str(i.split(\"\\t\")[1]).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user = {}\n",
    "for i in range(len(id_users_followers)):\n",
    "    user_followed = []\n",
    "    if id_users_followers[i] in user_user.keys():\n",
    "        user_followed = user_user[id_users_followers[i]]\n",
    "    user_followed.append(id_users_followed[i])\n",
    "    user_user[id_users_followers[i]] = user_followed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i> user_user : dico de la forme clé du dico suit tous les utilisateurs de la liste associé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_propa_article = []\n",
    "for i in df.index:\n",
    "    nb_propa_article.append(sum(article_users[str(i)].values()))\n",
    "df['nb_propa_article'] = nb_propa_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Ajout des topics des articles </u></b> \n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = MiniBatchKMeans(n_clusters=len(df))\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vec.get_feature_names()\n",
    "for i in range(len(df)):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Partie Réseau</u></b>\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Séparation de notre dataframe en deux df : train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df.loc[X_train_ids])\n",
    "df_test = pd.DataFrame(df.loc[X_test_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Partie entraînement du modèle</b></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Contribution de nos features\n",
    "\n",
    "X = df_train \n",
    "y = Y_train    \n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier(loss = 'exponential')\n",
    "model.fit(X,y)\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(30).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(loss = 'exponential')\n",
    "pred = np.mean(cross_val_score(model, df_train, Y_train, cv=10))\n",
    "print(pred)\n",
    "clf_gbc = GradientBoostingClassifier(loss = 'exponential')\n",
    "clf_gbc.fit(df_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Contribution de nos features\n",
    "\n",
    "X = df_train \n",
    "y = Y_train    \n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators= 1400, min_samples_split= 2, min_samples_leaf= 1, max_features='auto', max_depth= 40, bootstrap= False)\n",
    "model.fit(X,y)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(30).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators= 1400, min_samples_split= 2, min_samples_leaf= 1, max_features='auto', max_depth= 40, bootstrap= False)\n",
    "pred = np.mean(cross_val_score(model, df_train, Y_train, cv=10))\n",
    "print(pred)\n",
    "clf_gbc = RandomForestClassifier(n_estimators= 1400, min_samples_split= 2, min_samples_leaf= 1, max_features='auto', max_depth= 40, bootstrap= False)\n",
    "clf_gbc.fit(df_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Pour push sur Kaggle</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = clf_gbc.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"../pred_test.txt\")\n",
    "fichier = open(\"../pred_test.txt\", \"a\")\n",
    "fichier.write(\"doc,class\")\n",
    "for i in range(len(pred_test)):\n",
    "    fichier.write(\"\\n\" + str(X_test_ids[i]) + ',' + str(pred_test[i]))\n",
    "fichier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul du score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_100 = open(\"../pred_test_v4.txt\", \"r\").readlines()[1:]\n",
    "fichier_push = open(\"../pred_test.txt\", \"r\").readlines()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_a_avoir = []\n",
    "result_eu = []\n",
    "for i in fichier_100:\n",
    "    result_a_avoir.append(i.split(',')[1])\n",
    "for i in fichier_push:\n",
    "    result_eu.append(i.split(',')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(result_eu, result_a_avoir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
